"""
===================================================================
üöÄ ML-INTEGRATED CONVERSATION SYSTEM - COMPLETE STARTUP GUIDE
===================================================================

This is your complete LLM-to-LLM conversation system with integrated
machine learning for continuous improvement.

WHAT YOU HAVE:
  ‚úÖ User LLM (deepseek-r1:8b) - generates authentic user problems
  ‚úÖ Persona Prime (qwen3:14b) - provides wise guidance  
  ‚úÖ Domain Detection - analyzes problems automatically
  ‚úÖ Session Management - tracks conversation lifecycle
  ‚úÖ Episodic Memory - stores experiences for learning
  ‚úÖ ML Orchestration - analyzes patterns and improves
  ‚úÖ Full Integration - all components working together

===================================================================
QUICK START (3 STEPS)
===================================================================

1. Ensure Ollama is running:
   
   ollama serve

2. Run the system:
   
   python ml_integrated_conversation.py

3. Watch it run:
   
   Session 1 ‚Üí Dialogue with ML analysis ‚Üí Session 2 improved ‚Üí ...
   Press Ctrl+C to stop

===================================================================
WHAT HAPPENS DURING EXECUTION
===================================================================

Each Session:

  [Phase 1] Generate Problem
    ‚Ä¢ User LLM creates realistic problem
    ‚Ä¢ Example: "Should I change careers?"
  
  [Phase 2] Analyze Problem  
    ‚Ä¢ Domain detection: career, finance, health, etc.
    ‚Ä¢ Stakes: low/medium/high
    ‚Ä¢ Reversibility: reversible/partially/irreversible
  
  [Phase 3] Start Session Management
    ‚Ä¢ SessionManager tracks start time, problem, domains
    ‚Ä¢ Creates unique session ID
  
  [Phase 4] Run Dialogue
    TURN 1:
      Persona Prime: "Let me understand your situation..."
      User LLM: "Here's what's happening..."
      [Satisfaction check]
    
    TURN 2+:
      Persona Prime: "Can you tell me more about..."
      User LLM: "Yes, specifically..."
      [Satisfaction check]
    
    Until: User LLM is SATISFIED or max turns reached
  
  [Phase 5] Store & ML Analysis
    ‚Ä¢ Save conversation to JSON
    ‚Ä¢ Store episode in episodic memory
    ‚Ä¢ Record performance metrics
    ‚Ä¢ üß† ML analyzes outcomes
      ‚îú‚îÄ Extract domain effectiveness
      ‚îú‚îÄ Assess conversation quality
      ‚îú‚îÄ Identify weak domains
      ‚îî‚îÄ Generate recommendations
  
  [Phase 6] Session Complete
    ‚Ä¢ Record satisfaction level
    ‚Ä¢ Print learning summary
    ‚Ä¢ Ready for next session

===================================================================
OUTPUT YOU'LL SEE
===================================================================

Session starts:

  ========================================================================
  SESSION 1 - ML-Integrated Conversation
  ========================================================================

  [Phase 1] Generating user problem...
  [PROBLEM]
  I've been offered a job promotion that requires relocating...

  [Phase 2] Analyzing problem for domains...
  Domains: career, lifestyle
  Stakes: high

  [Phase 3] Starting session management...
  [Phase 4] Running dialogue with ML integration...

    [TURN 1]
      [Persona Prime] Thinking...
      [Response] Understanding your situation. Let me ask...
      [User LLM] Reacting...
      [User] Yes, and my family is concerned about...
      [Evaluating] Satisfaction check...
      ‚ö†Ô∏è Partial satisfaction, continuing...

    [TURN 2]
      [Persona Prime] Thinking...
      ...continues...

After dialogue completes:

  ========================================================================
  ML LEARNING ANALYSIS
  ========================================================================

  [Metrics]
    ‚Ä¢ Turns: 3
    ‚Ä¢ Satisfied: YES  
    ‚Ä¢ Confidence: 87%
    ‚Ä¢ Domains: career, lifecycle

  [Analysis]
    ‚Ä¢ Conversation Depth: 6 exchanges
    ‚Ä¢ Satisfaction Indicator: high
    ‚Ä¢ Pattern: Quick resolution (efficient)

  Learning persisted to:
    ‚Üí data/memory/episodes.jsonl
    ‚Üí data/memory/metrics.jsonl

  [Status] Session 1 complete. Starting session 2...


After all sessions (press Ctrl+C):

  ========================================================================
  SESSIONS COMPLETE
  ========================================================================

  Total sessions: 5
  Total turns: 23
  Avg turns/session: 4.6
  Learning records: 5
  Satisfied sessions: 4/5 (80%)

  [Data Stored]
    ‚Ä¢ Conversations: data/conversations/
    ‚Ä¢ Episodes: data/memory/episodes.jsonl
    ‚Ä¢ Metrics: data/memory/metrics.jsonl
    ‚Ä¢ Sessions: data/sessions/

===================================================================
HOW ML LEARNING IMPROVES THE SYSTEM
===================================================================

Session 1 (Career):
  Problem: "Should I change jobs?"
  Result: SATISFIED in 3 turns, 87% confidence
  Learning: "Career domain responds well to structured questions"

Session 2 (Psychology):
  Problem: "How do I handle family conflict?"
  Result: UNSATISFIED after 5 turns, 65% confidence
  Learning: "Psychology is complex - needs different approach"
  Flagged: Weak domain

Session 3 (Career again):
  Problem: "Should I freelance or stay employed?"
  ML Access: "We know this domain works well"
  Applied: Same successful pattern from Session 1
  Result: SATISFIED in 3 turns, 88% confidence
  Improvement: ‚úÖ System used learned pattern

Session 4 (Psychology again):
  Problem: "Should I set boundaries with family?"
  ML Access: "This domain is weak - use different approach"
  Tried: Extended dialogue + extra perspective
  Result: PARTIAL SATISFIED in 4 turns, 72% confidence
  Improvement: ‚úÖ Used different approach, got improvement

PATTERN: Sessions 3+ improve because system learns from 1-2


===================================================================
SYSTEM ARCHITECTURE
===================================================================

User Problem Generation
        ‚Üì
Domain Analysis (15 domains detected)
        ‚Üì
Session Initialization
        ‚îú‚îÄ Problem statement
        ‚îú‚îÄ Detected domains
        ‚îú‚îÄ Stakes level
        ‚îî‚îÄ Reversibility
        ‚Üì
Conversation Loop
  ‚îú‚îÄ Persona Prime asks clarifying questions
  ‚îú‚îÄ User LLM provides authentic responses
  ‚îú‚îÄ Satisfaction check after each turn
  ‚îî‚îÄ Repeat until satisfied or max turns
        ‚Üì
Episode Storage (Episodic Memory)
        ‚îú‚îÄ User input
        ‚îú‚îÄ Recommendation
        ‚îú‚îÄ Confidence level
        ‚îú‚îÄ Outcome
        ‚îî‚îÄ Regret score
        ‚Üì
Metrics Recording (Performance Metrics)
        ‚îú‚îÄ Turns needed
        ‚îú‚îÄ Domain
        ‚îú‚îÄ Confidence
        ‚îú‚îÄ Outcome
        ‚îî‚îÄ Success indicator
        ‚Üì
ML Analysis Pipeline
        ‚îú‚îÄ Extract metrics
        ‚îú‚îÄ Analyze domain effectiveness
        ‚îú‚îÄ Assess conversation quality
        ‚îú‚îÄ Identify weak domains (system-wide)
        ‚îî‚îÄ Generate recommendations for next session
        ‚Üì
Learning Insights Saved
        ‚îú‚îÄ learning-insights.jsonl (cumulative)
        ‚îî‚îÄ weak-domains.json (summary)
        ‚Üì
Next Session Uses Learned Patterns ‚Üí System Improves


===================================================================
DATA STORAGE LOCATIONS
===================================================================

Generated during execution:

  data/conversations/
    ‚îî‚îÄ uc_<timestamp>_s<num>.json
       Full conversation transcripts with dialogue

  data/sessions/
    ‚îú‚îÄ completed/
    ‚îÇ   ‚îî‚îÄ session_<id>.json
    ‚îÇ       Session metadata, problem, domains
    ‚îî‚îÄ consequences.jsonl
        Follow-up outcomes, learning integration

  data/memory/
    ‚îú‚îÄ episodes.jsonl
    ‚îÇ   Episode objects: decisions, outcomes, consequences
    ‚îî‚îÄ metrics.jsonl
        Performance metrics: turns, confidence, success rates

  ml/cache/
    ‚îú‚îÄ outcomes/
    ‚îÇ   Outcome recording and analysis
    ‚îî‚îÄ training_datasets/
        ML training data extracted from conversations


===================================================================
KEY FEATURES
===================================================================

‚úÖ Automatic Domain Detection
   15 domains: career, finance, health, relationships, psychology,
   education, legal, ethical, technical, creative, family, personal,
   spiritual, community, lifestyle

‚úÖ Intelligent Conversation Flow
   ‚Ä¢ Clarifying questions in turn 1
   ‚Ä¢ Context-aware responses in turns 2+
   ‚Ä¢ Satisfaction checks after each turn
   ‚Ä¢ Optional final synthesis

‚úÖ Multi-LLM Orchestration
   ‚Ä¢ User LLM: Generates authentic human responses
   ‚Ä¢ Persona Prime: Provides wise, consistent guidance
   ‚Ä¢ Independent reasoning, collaborative outcomes

‚úÖ Episodic Learning System
   ‚Ä¢ Every conversation stored as episode
   ‚Ä¢ Decisions tracked with outcomes
   ‚Ä¢ Consequences recorded over time
   ‚Ä¢ Used for pattern extraction and improvement

‚úÖ Performance Metrics
   ‚Ä¢ Success rate per domain
   ‚Ä¢ Average turns needed
   ‚Ä¢ Confidence levels achieved
   ‚Ä¢ Weak domain identification

‚úÖ ML Wisdom Integration
   ‚Ä¢ Pattern analysis from conversations
   ‚Ä¢ Domain effectiveness tracking
   ‚Ä¢ Recommendation quality assessment
   ‚Ä¢ System-wide weak domain detection


===================================================================
TROUBLESHOOTING
===================================================================

ISSUE: "Ollama not reachable"
SOLUTION: 
  Start Ollama: ollama serve
  Models needed: deepseek-r1:8b, qwen3:14b
  Check: ollama list

ISSUE: "Module not found"
SOLUTION:
  Check all imports work: python quick_test_ml.py
  Ensure persona/ and ml/ folders exist
  Verify __init__.py files present

ISSUE: "Session manager error"
SOLUTION:
  Create directories: mkdir -p data/sessions data/memory
  Check file permissions
  Verify session_manager.py accessible

ISSUE: "ML analysis fails"
SOLUTION:
  Check ml/cache/ exists
  Verify episodic_memory.py working
  Run quick_test_ml.py to diagnose


===================================================================
NEXT STEPS
===================================================================

1. START THE SYSTEM
   
   python ml_integrated_conversation.py

2. MONITOR LEARNING
   
   Watch learning summaries after each conversation
   Look for weak domain patterns

3. ANALYZE DATA
   
   # View weak domains
   cat data/learning_insights/weak-domains.json
   
   # View episodes
   tail -5 data/memory/episodes.jsonl
   
   # View metrics
   tail -5 data/memory/metrics.jsonl

4. OBSERVE IMPROVEMENT
   
   Sessions 1-2: Establish baselines
   Sessions 3-5: See improvement as patterns emerge
   Sessions 5+: System increasingly effective


===================================================================
VERIFICATION
===================================================================

Quick test (before running full system):

  python quick_test_ml.py

Shows:
  ‚úÖ All imports working
  ‚úÖ System initializes
  ‚úÖ Components verified
  ‚úÖ Ready to use


===================================================================
STATUS: READY TO USE ‚úÖ
===================================================================

All components integrated and verified.

Run: python ml_integrated_conversation.py

Watch the system learn and improve through conversation!

===================================================================
"""

if __name__ == "__main__":
    print(__doc__)
