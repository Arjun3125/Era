# 04_DATA_FLOW.md

# ğŸ”„ Era Project - Data Flow Documentation

**How data moves through the system, from user input to learned wisdom**

---

## Overview

This document traces data flow through three main pipelines:

1. **Decision Pipeline** - User input â†’ Persona response
2. **Learning Pipeline** - Outcome â†’ ML training â†’ Improvement
3. **Memory Pipeline** - Real-time â†’ Validated â†’ Long-term storage

---

## Pipeline 1: Decision Flow

### High-Level Flow

```
User Input
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Mode Check     â”‚
â”‚  (/mode cmd?)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Get Current Modeâ”‚
â”‚ QUICK/WAR/      â”‚
â”‚ MEETING/DARBAR  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Mode Orchestrator          â”‚
â”‚  Routes Decision            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  QUICK  â”‚ â”‚ WAR/MEETING/     â”‚
â”‚  Mode   â”‚ â”‚ DARBAR           â”‚
â”‚         â”‚ â”‚                  â”‚
â”‚ Direct  â”‚ â”‚ Dynamic Council  â”‚
â”‚ LLM     â”‚ â”‚ â”œâ”€ Select Mins   â”‚
â”‚ Responseâ”‚ â”‚ â”œâ”€ Convene       â”‚
â”‚         â”‚ â”‚ â”œâ”€ Aggregate     â”‚
â”‚         â”‚ â”‚ â””â”€ Prime Review  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ KIS Ranking    â”‚
    â”‚ (Knowledge     â”‚
    â”‚  Integration)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ML Judgment    â”‚
    â”‚ Prior Applied  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Generate       â”‚
    â”‚ Response       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Display to     â”‚
    â”‚ User           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Step-by-Step

#### Step 1: Input Reception
```
Data: User input string
Example: "Should I quit my job to start a company?"

Flow:
  main.py:main() â†’ receives input
  context.py:update_context() â†’ adds to conversation history
```

#### Step 2: Mode Check
```
Data: Input string
Check: Does input start with "/mode"?

If YES:
  mode_orchestrator.py:select_mode() â†’ switch mode
  Return to Step 1

If NO:
  Continue to Step 3
```

#### Step 3: LLM Handshakes (Sensing)
```
Input: User input string
Output: Situation + Constraints + Counterfactuals + Intent

Calls:
  llm_interface.py:run_handshake_sequence()
  
  CALL 1+2 (Merged): Situation + Constraints
    â†’ decision_type: "irreversible"
    â†’ risk_level: "high"
    â†’ time_horizon: "long"
    â†’ irreversibility_score: 0.9
    â†’ fragility_score: 0.7
    
  CALL 3: Counterfactual Sketch
    â†’ Option 1: Quit now (downside: no income)
    â†’ Option 2: Wait 6 months (downside: miss opportunity)
    â†’ Option 3: Start side hustle (downside: burnout)
    
  CALL 4: Intent Detection
    â†’ goal_orientation: "achievement"
    â†’ emotional_pressure: 0.6
    â†’ urgency_bias: "high"
```

#### Step 4: Mode Routing
```
Input: Current mode, domain, user input
Output: Council recommendation (or direct LLM)

Flow:
  mode_orchestrator.py:route_decision()
  
  If QUICK:
    â†’ Skip council
    â†’ Direct LLM call
    â†’ Return response
    
  If WAR/MEETING/DARBAR:
    â†’ dynamic_council.py:select_ministers()
    â†’ Select based on mode + domain
    â†’ Proceed to Step 5
```

#### Step 5: Minister Selection (WAR/MEETING/DARBAR)
```
Input: Mode, domain
Output: List of ministers to convene

Selection Logic:
  WAR: [Risk, Power, Strategy, Technology, Timing] (fixed 5)
  MEETING: 3-5 domain-relevant ministers
  DARBAR: All 18 ministers

Example (MEETING, domain=career):
  Selected: [Career, Risk, Economics, Psychology, Strategy]
```

#### Step 6: Council Convening
```
Input: Selected ministers, user input, context
Output: Minister recommendations + votes

Flow:
  dynamic_council.py:convene_council()
  
  For each minister:
    â†’ Load minister doctrine
    â†’ Query KIS for relevant knowledge
    â†’ Generate recommendation
    â†’ Cast vote (confidence-scored)
    
  Output:
    {
      "recommendations": [...],
      "votes": {...},
      "consensus_strength": 0.75
    }
```

#### Step 7: KIS Ranking
```
Input: Domain, active_domains, user_input
Output: Top-N knowledge entries with scores

Flow:
  knowledge_integration_system.py:synthesize_knowledge()
  
  For each knowledge entry:
    â†’ Compute domain_weight (0.25-1.4)
    â†’ Compute type_weight (0.9-1.1)
    â†’ Compute memory_weight (1.0-8.0)
    â†’ Compute context_weight (0.85-1.4)
    â†’ Compute goal_weight (0.7-1.2)
    â†’ KIS_score = product of all weights
    
  Sort by KIS_score
  Return top 5-10 entries
```

#### Step 8: ML Judgment Prior Application
```
Input: Situation features
Output: Adjusted knowledge type weights

Flow:
  ml_judgment_prior.py:predict_prior()
  
  1. Extract situation features
  2. Compute situation_hash
  3. Look up learned priors for hash
  4. If confidence > 0.6:
       â†’ Return priors
     Else:
       â†’ Return neutral weights
       
  Apply:
    adjusted_KIS = KIS Ã— ml_prior_weight
```

#### Step 9: Aggregation
```
Input: Minister recommendations, KIS results
Output: Aggregated recommendation

Flow:
  dynamic_council.py:aggregate_recommendations()
  
  Mode-specific aggregation:
    WAR: Victory-focused synthesis
    MEETING: Balanced multi-perspective
    DARBAR: Full doctrine-driven deliberation
    
  Output:
    {
      "recommendation": "...",
      "confidence": 0.75,
      "minister_votes": {...},
      "dissenting_opinions": [...]
    }
```

#### Step 10: Prime Confident Review
```
Input: Council recommendation
Output: Approved/rejected/modified recommendation

Flow:
  prime_confident.py:review_recommendation()
  
  Checks:
    â†’ Doctrine alignment
    â†’ Red-line protection
    â†’ Logical consistency
    
  If approved:
    â†’ Return recommendation
  If rejected:
    â†’ Override with alternative
  If modified:
    â†’ Return modified version
```

#### Step 11: Response Generation
```
Input: Final recommendation, mode, context
Output: Natural language response

Flow:
  ollama_runtime.py:generate()
  
  Prompt includes:
    â†’ User input
    â†’ Recommendation
    â†’ Minister input (if council used)
    â†’ Mode framing
    â†’ Doctrine context
    
  Model: qwen3:14b
  Timeout: 30 seconds
```

#### Step 12: Display
```
Input: Response string
Output: Displayed to user

Format:
  N: [MODE] "Response text..."
  
Example:
  N: [MEETING] "Council convenes. Risk Minister expresses
      concern about cash flow. Strategy Minister suggests
      a phased approach..."
```

---

## Pipeline 2: Learning Flow

### High-Level Flow

```
Decision Made
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Store Episode   â”‚
â”‚ (Episodic Mem)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Record Metrics  â”‚
â”‚ (Performance)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Observe Outcome â”‚
â”‚ (Success/Fail)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate Label  â”‚
â”‚ (Type Weights)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Add to Training â”‚
â”‚ (Batch Buffer)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Train Model     â”‚
â”‚ (Every 50)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extract Patternsâ”‚
â”‚ (Every 100)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retrain System  â”‚
â”‚ (Every 200)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Step-by-Step

#### Step 1: Episode Storage (Every Turn)
```
Input: Turn data (decision, confidence, outcome)
Output: Episode stored in JSONL

Data Structure:
  {
    "turn_id": 150,
    "timestamp": "2026-02-18T02:30:00Z",
    "domain": "career",
    "mode": "MEETING",
    "user_input": "...",
    "recommendation": "...",
    "confidence": 0.75,
    "minister_votes": {...},
    "council_recommendation": "...",
    "outcome": "success" | "failure",
    "regret_score": 0.0-1.0,
    "consequences": [...]
  }

Storage:
  Memory/YYYY-MM-DD_episodes.jsonl
  (append one line per turn)
```

#### Step 2: Metrics Recording (Every Turn)
```
Input: Turn data, outcome
Output: Updated metrics

Flow:
  performance_metrics.py:record_decision()
  
  Updates:
    â†’ Domain success rate
    â†’ Minister performance
    â†’ Mode stability
    â†’ Feature coverage
    
  Stored in:
    live_metrics.json (in-memory, flushed every 10 turns)
```

#### Step 3: Outcome Observation
```
Input: User reaction, consequence data
Output: Outcome classification

Classification:
  Success: User satisfied, positive consequences
  Failure: User regretted, negative consequences
  
Regret Score:
  0.0 = No regret (perfect outcome)
  0.3 = Mild regret
  0.6 = Moderate regret
  0.8+ = Severe regret
```

#### Step 4: Label Generation
```
Input: Situation features, outcome, regret
Output: Training label (type weight adjustments)

Flow:
  label_generator.py:generate_type_weights()
  
  Logic:
    If failure + irreversibility:
      â†’ warning_weight += 0.2
      â†’ principle_weight += 0.1
      
    If failure + rule-heavy:
      â†’ rule_weight -= 0.15
      
    If success + irreversible:
      â†’ principle_weight += 0.15
      
    If advice + high regret:
      â†’ advice_weight -= 0.2
      
  Clamp all weights to [0.7, 1.3]
  
  Output Label:
    {
      "principle_weight": 1.2,
      "rule_weight": 0.95,
      "warning_weight": 1.15,
      "claim_weight": 1.0,
      "advice_weight": 0.85
    }
```

#### Step 5: Training Sample Accumulation
```
Input: Features + Label
Output: Training sample added to buffer

Data Structure:
  {
    "features": {...},  # 41-dim vector
    "label": {...},     # Type weights
    "situation_hash": "irreversible_high_h",
    "timestamp": "..."
  }

Buffer Size: 50 samples
When full â†’ Trigger training
```

#### Step 6: Model Training (Every 50 Samples)
```
Input: 50 training samples
Output: Updated judgment prior model

Flow:
  ml_judgment_prior.py:train()
  
  Algorithm:
    1. Group samples by situation_hash
    2. For each hash group:
       â†’ Compute average type weights
       â†’ Store as prior for that situation type
    3. Save model to disk
    
  Model Storage:
    ml/models/judgment_prior.json
```

#### Step 7: Pattern Extraction (Every 100 Turns)
```
Input: Episodic memory (100+ episodes)
Output: Identified patterns and clusters

Flow:
  pattern_extraction.py:extract_patterns()
  
  Patterns Detected:
    1. long_failure_streak
       â†’ 3+ consecutive failures
       â†’ Triggers alert
       
    2. high_regret_cluster
       â†’ Multiple high-regret decisions
       â†’ Indicates systemic issue
       
    3. weak_domain_pattern
       â†’ Domain <50% success rate
       â†’ Triggers retraining
       
    4. minister_underperformance
       â†’ Specific minister consistently wrong
       â†’ Triggers minister retraining
       
  Output:
    {
      "patterns": [...],
      "learning_signals": [...],
      "recommendations": [...]
    }
```

#### Step 8: PWM Sync (Every 100 Turns)
```
Input: Episodic memory, Metrics snapshot
Output: Validated facts committed to PWM

Flow:
  pwm_bridge.py:periodic_pwm_sync()
  
  Validation:
    â†’ Check consistency across episodes
    â†’ Verify with metrics
    â†’ Ensure confidence > 0.75
    
  If validated:
    pwm.update_entity(
      entity="john",
      field="risk_tolerance",
      value=0.4,
      confidence=0.85
    )
    
  PWM Storage:
    Entity-attribute graph
    (high-confidence facts only)
```

#### Step 9: System Retraining (Every 200 Turns)
```
Input: Patterns, PWM insights, Metrics
Output: Updated ministers, doctrines, KIS weights

Flow:
  system_retraining.py:retrain_all()
  
  Steps:
    1. Extract success patterns
    2. Update minister confidence formulas
    3. Evolve doctrines from patterns
    4. Rebalance KIS weights
    5. Apply PWM insights
    
  Minister Update:
    For each domain:
      â†’ Identify weak ministers
      â†’ Update confidence formulas
      â†’ Apply learned doctrine
      
  Doctrine Update:
    â†’ Add new principles from patterns
    â†’ Adjust rules based on outcomes
    â†’ Update warnings from failures
```

---

## Pipeline 3: Memory Flow

### Three-Tier Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MEMORY FLOW DIAGRAM                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Every Turn:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Episodic     â”‚ â† Fast, detailed, observed
â”‚ Memory       â”‚   (decision + outcome)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Aggregate
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Performance  â”‚ â† Medium, statistical
â”‚ Metrics      â”‚   (success rates)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Validate (100 turns)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PWM          â”‚ â† Slow, validated, high-confidence
â”‚ (Personal    â”‚   (stable facts about person)
â”‚  World Model)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tier 1: Episodic Memory (Fast)

**Update Frequency:** Every turn  
**Storage Format:** JSONL  
**Location:** `Memory/YYYY-MM-DD_episodes.jsonl`  
**Purpose:** Pattern detection, mistake prevention  

**Data Flow:**
```
Turn N Complete
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Build Episode Object                â”‚
â”‚ {                                   â”‚
â”‚   turn_id, timestamp, domain,       â”‚
â”‚   mode, user_input, recommendation, â”‚
â”‚   confidence, minister_votes,       â”‚
â”‚   outcome, regret_score,            â”‚
â”‚   consequences                      â”‚
â”‚ }                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Append to JSONL File                â”‚
â”‚ Memory/YYYY-MM-DD_episodes.jsonl    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Query Patterns:**
```python
# Find similar episodes
similar = episodic.find_similar_episodes(
    domain="career",
    pattern="quit_without_buffer"
)

# Get recent episodes
recent = episodic.get_recent_episodes(
    count=10,
    domain="finance"
)

# Extract lessons
lessons = episodic.extract_lessons(
    domain="career",
    min_confidence=0.7
)
```

---

### Tier 2: Performance Metrics (Medium)

**Update Frequency:** Every turn (recorded), every 100 turns (aggregated)  
**Storage Format:** JSON  
**Location:** `live_metrics.json`  
**Purpose:** Identify weak domains, guide retraining  

**Data Flow:**
```
Turn N Complete
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Update In-Memory Metrics            â”‚
â”‚ - Domain success rate               â”‚
â”‚ - Minister performance              â”‚
â”‚ - Mode stability                    â”‚
â”‚ - Feature coverage                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (Every 10 turns)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flush to Disk                       â”‚
â”‚ live_metrics.json                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (Every 100 turns)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aggregate & Analyze                 â”‚
â”‚ - Compute rolling success rates     â”‚
â”‚ - Detect weak domains (<50%)        â”‚
â”‚ - Compute minister adjustments      â”‚
â”‚ - Generate periodic report          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Metrics Structure:**
```json
{
  "domains": {
    "career": {
      "success_rate": 0.67,
      "turn_count": 45,
      "avg_confidence": 0.75,
      "avg_regret": 0.3
    },
    "finance": {
      "success_rate": 0.55,
      "turn_count": 30,
      "avg_confidence": 0.68,
      "avg_regret": 0.4
    }
  },
  "ministers": {
    "risk": {
      "accuracy": 0.72,
      "avg_confidence": 0.8
    },
    "strategy": {
      "accuracy": 0.68,
      "avg_confidence": 0.75
    }
  },
  "modes": {
    "QUICK": {"success_rate": 0.65, "turn_count": 20},
    "WAR": {"success_rate": 0.70, "turn_count": 15},
    "MEETING": {"success_rate": 0.67, "turn_count": 50},
    "DARBAR": {"success_rate": 0.75, "turn_count": 15}
  },
  "overall": {
    "success_rate": 0.67,
    "total_turns": 100,
    "improvement": "+16.7%"
  }
}
```

---

### Tier 3: PWM - Personal World Model (Slow)

**Update Frequency:** Every 100 turns (after validation)  
**Storage Format:** Entity-attribute graph  
**Location:** `data/pwm/entities.json`  
**Purpose:** Stable, high-confidence facts  

**Data Flow:**
```
Turn 100, 200, 300...
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Collect Metrics Snapshot            â”‚
â”‚ - Domain success rates              â”‚
â”‚ - Minister performance              â”‚
â”‚ - Pattern analysis                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Validate Observations               â”‚
â”‚ - Check consistency                 â”‚
â”‚ - Verify confidence > 0.75          â”‚
â”‚ - Cross-reference episodes          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Commit Validated Facts to PWM       â”‚
â”‚ pwm.update_entity(                  â”‚
â”‚   entity="john",                    â”‚
â”‚   field="risk_tolerance",           â”‚
â”‚   value=0.4,                        â”‚
â”‚   confidence=0.85                   â”‚
â”‚ )                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate Actionable Insights        â”‚
â”‚ pwm.generate_actionable_insights()  â”‚
â”‚ â†’ ["John is risk-averse",           â”‚
â”‚    "Prefers email communication",   â”‚
â”‚    "Values work-life balance"]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**PWM Structure:**
```json
{
  "entities": {
    "john": {
      "attributes": {
        "risk_tolerance": {
          "value": 0.4,
          "confidence": 0.85,
          "last_updated": "turn_200",
          "source": "metrics_validation"
        },
        "communication_preference": {
          "value": "email",
          "confidence": 0.9,
          "last_updated": "turn_100",
          "source": "pattern_analysis"
        }
      },
      "relationships": {
        "alice": {
          "type": "spouse",
          "trust_level": 0.8,
          "dynamics": "supportive"
        }
      }
    }
  },
  "timeline": [
    {
      "turn": 100,
      "event": "entity_created",
      "entity": "john"
    },
    {
      "turn": 200,
      "event": "attribute_updated",
      "entity": "john",
      "field": "risk_tolerance",
      "old_value": 0.6,
      "new_value": 0.4
    }
  ]
}
```

---

## Cross-Pipeline Data Flows

### Decision â†’ Learning â†’ Memory

```
Decision Made (Pipeline 1)
    â”‚
    â”œâ”€â†’ Store Episode (Pipeline 3, Tier 1)
    â”‚
    â”œâ”€â†’ Record Metrics (Pipeline 3, Tier 2)
    â”‚
    â””â”€â†’ Observe Outcome
         â”‚
         â–¼
    Generate Label (Pipeline 2)
         â”‚
         â–¼
    Train ML Model (Pipeline 2)
         â”‚
         â–¼
    Extract Patterns (Pipeline 2)
         â”‚
         â–¼
    Retrain System (Pipeline 2)
         â”‚
         â–¼
    Update Ministers (affects Pipeline 1)
    Update Doctrines (affects Pipeline 1)
    Update KIS Weights (affects Pipeline 1)
```

### Memory â†’ Decision Enhancement

```
Episodic Memory (Tier 1)
    â”‚
    â”œâ”€â†’ Find Similar Episodes
    â”‚   â””â”€â†’ Inform current decision
    â”‚
    â””â”€â†’ Extract Lessons
         â””â”€â†’ Update doctrines
              â””â”€â†’ Affect future decisions

Performance Metrics (Tier 2)
    â”‚
    â”œâ”€â†’ Detect Weak Domains
    â”‚   â””â”€â†’ Trigger retraining
    â”‚
    â””â”€â†’ Compute Minister Adjustments
         â””â”€â†’ Update minister confidence
              â””â”€â†’ Affect voting weights

PWM (Tier 3)
    â”‚
    â””â”€â†’ Generate Actionable Insights
         â””â”€â†’ Inform persona responses
              â””â”€â†’ More personalized advice
```

---

## Data Storage Summary

| Data Type | Format | Location | Update Freq | Size/Turn |
|-----------|--------|----------|-------------|-----------|
| Episodes | JSONL | `Memory/YYYY-MM-DD.jsonl` | Every turn | ~500 bytes |
| Metrics | JSON | `live_metrics.json` | Every 10 turns | ~5 KB |
| ML Model | JSON | `ml/models/judgment_prior.json` | Every 50 samples | ~10 KB |
| PWM | JSON | `data/pwm/entities.json` | Every 100 turns | ~2 KB |
| Patterns | JSON | `Memory/patterns.json` | Every 100 turns | ~1 KB |
| Logs | TXT | `logs/*.log` | Continuous | ~100 bytes/turn |

**Total Storage Growth:** ~600 bytes per turn  
**1000 Turns:** ~600 KB  
**10000 Turns:** ~6 MB

---

## Data Flow Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     COMPLETE DATA FLOW                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

USER INPUT
    â”‚
    â”œâ”€â”€â”€â†’ [LLM Handshakes] â”€â”€â”€â†’ Situation + Constraints
    â”‚                                  â”‚
    â”œâ”€â”€â”€â†’ [Mode Orchestrator] â”€â”€â†’ Mode Decision
    â”‚                                  â”‚
    â”œâ”€â”€â”€â†’ [Dynamic Council] â”€â”€â”€â†’ Minister Recommendations
    â”‚                                  â”‚
    â”œâ”€â”€â”€â†’ [KIS Engine] â”€â”€â”€â”€â”€â”€â”€â”€â†’ Knowledge Rankings
    â”‚                                  â”‚
    â”œâ”€â”€â”€â†’ [ML Judgment Prior] â”€â†’ Adjusted Weights
    â”‚                                  â”‚
    â””â”€â”€â”€â†’ [Prime Confident] â”€â”€â”€â†’ Final Approval
                                     â”‚
                                     â–¼
                              PERSONA RESPONSE
                                     â”‚
                                     â”œâ”€â”€â”€â†’ Display to User
                                     â”‚
                                     â”œâ”€â”€â”€â†’ Store Episode â”€â”€â”€â†’ Episodic Memory
                                     â”‚
                                     â”œâ”€â”€â”€â†’ Record Metrics â”€â”€â†’ Performance Metrics
                                     â”‚
                                     â””â”€â”€â”€â†’ Observe Outcome
                                            â”‚
                                            â”œâ”€â”€â”€â†’ Generate Label
                                            â”‚        â”‚
                                            â”‚        â–¼
                                            â”‚   Train ML Model
                                            â”‚        â”‚
                                            â”‚        â–¼
                                            â”‚   Update Priors
                                            â”‚
                                            â””â”€â”€â”€â†’ Extract Patterns (100 turns)
                                                   â”‚
                                                   â”œâ”€â”€â”€â†’ Retrain Ministers (200 turns)
                                                   â”‚
                                                   â””â”€â”€â”€â†’ Sync PWM (100 turns)
                                                          â”‚
                                                          â””â”€â”€â”€â†’ Generate Insights
                                                                 â”‚
                                                                 â””â”€â”€â”€â†’ Inform Future Decisions
```

---

ğŸ“„ **Next:** [`05_FLOWCHARTS.md`](./05_FLOWCHARTS.md) - Visual diagrams (Mermaid + ASCII)
