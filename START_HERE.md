# ERA System - Quick Start Guide

Welcome to the **Excellent Reasoning Architecture (ERA)**—a sovereign decision governance system with ministerial councils, machine learning wisdom, and interactive dialogue.

## What is ERA?

ERA is a **Ministerial Cognitive Architecture (MCA)** that combines:
- **Decision Guidance**: 4-mode orchestration (QUICK → MEETING → WAR → DARBAR)
- **Wisdom Council**: 19 domain-bounded ministers giving advice
- **ML Learning**: Judgment priors trained from past decisions
- **KIS Engine**: Knowledge Integration System with 40K+ doctrine items
- **LLM Interaction**: Direct dialogue between you, the system, and LLM agents
- **Session Management**: Multi-turn problem solving with auto-escalation

---

## Installation

### Requirements
- Python 3.10+
- Ollama (for LLM inference)
- ~5GB disk space for models & data

### Setup

```bash
# 1. Navigate to ERA directory
cd c:\era

# 2. Create virtual environment (optional but recommended)
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# 3. Install dependencies
pip install -r requirements.txt

# 4. Verify Ollama is running
# Start Ollama in another terminal: ollama serve

# 5. Pull required models (one-time)
ollama pull deepseek-r1:8b
ollama pull qwen3:14b
```

---

## Running ERA

### ✅ Option 1: LLM Conversation (Easiest - Start Here!)

Have two LLMs discuss any topic:

```bash
# Interactive mode - you choose topics
python llm_conversation.py

# Demo mode - pre-built conversation
python llm_conversation.py --mode demo --rounds 3

# Specific topic
python llm_conversation.py --mode topic --topic "AI ethics in healthcare" --rounds 5
```

**What you'll see:**
- User LLM (deepseek-r1:8b) opens the conversation
- Program LLM (qwen3:14b) responds
- Back-and-forth dialogue saved to `data/conversations/`

**Commands in interactive mode:**
- Enter a topic to start
- `save` - Save current conversation
- `show` - Display full conversation
- `clear` - Start fresh
- `quit` - Exit

---

### ✅ Option 2: Decision Guidance System (Full Capabilities)

Get guidance on a problem through the 9-phase dialogue pipeline:

```bash
# Run the main system
python system_main.py

# Or use interactive conversation with sessions
python run_session_conversation.py
```

**What happens:**
1. **Phase 1:** Define your problem
2. **Phases 2-4:** System asks clarifying questions (3 rounds)
3. **Phase 5:** Synthesizes context with clarity
4. **Phase 6:** Gets User LLM feedback
5. **Phase 7:** Assesses your satisfaction
6. **Phases 8-9:** Stores episode for learning

---

### ✅ Option 3: Sovereign Council Simulation

Run a multi-agent simulation with synthetic humans:

```bash
python sovereign_main.py
```

This runs a complete decision scenario with:
- Real humans (via input prompts)
- Synthetic humans (generated by HSE - Human Simulation Engine)
- Full ministerial councils voting
- Outcome tracking and metrics

---

## Mode Selection Guide

**Use QUICK mode** (direct LLM, no council) when:
- Quick advice needed (mentoring style)
- Simple, low-stakes decisions
- Time-constrained situations
- Default: First 1-2 turns

**Use MEETING mode** (3-5 relevant ministers) when:
- Balanced debate needed
- Multi-perspective input wanted
- Medium stakes
- Default: Turns 3-5, auto-escalates after

**Use WAR mode** (5 specialized ministers) when:
- Victory-focused assessment
- Risk/strategy analysis needed
- High-stakes scenarios
- Default: Turns 6-8

**Use DARBAR mode** (all 19 ministers + judge) when:
- Seeking full wisdom council
- Unprecedented decisions
- Highest stakes situations
- Default: Turns 9+

→ See [MODE_QUICK_REFERENCE.md](documentation/MODE_QUICK_REFERENCE.md) for details

---

## Core Data Directories

```
data/
├── sessions/          # Session records & consequences
├── conversations/     # LLM conversation transcripts
├── doctrine/         # Minister decision-making rules
├── memory/           # Episodic learning storage
├── books/           # Ingested knowledge (61 documents)
└── RAG/             # Vector embeddings for knowledge
```

All paths are validated on startup. Missing directories will error with clear messages.

---

## Key Files & Entry Points

| File | Purpose | Command |
|------|---------|---------|
| `llm_conversation.py` | LLM dialogue engine | `python llm_conversation.py` |
| `system_main.py` | Decision guidance (9-phase) | `python system_main.py` |
| `run_session_conversation.py` | Multi-session problem solving | `python run_session_conversation.py` |
| `sovereign_main.py` | Ministerial council simulation | `python sovereign_main.py` |
| `test_ml_layer.py` | ML wisdom system validation | `python test_ml_layer.py` |

---

## Understanding the System Architecture

### Three Core Layers

**1. Mode Orchestrator** (persona/modes/)
- Routes decisions through 4 organizational modes
- Selects relevant ministers based on decision type
- Tracks decision complexity and escalates as needed

**2. Ministerial Council** (sovereign/ministers/)
- 19 domain-bounded advisors (Risk, Data, Diplomacy, etc.)
- Each generates KIS (Knowledge Integration System) in their domain
- Provides positions to Prime Confident for final decision

**3. ML Wisdom System** (ml/)
- Layer 1: LLM Handshakes - Situation framing & constraint extraction
- Layer 2: Feature Extraction - Decision state → numeric vectors
- Layer 3: KIS - 5-factor knowledge scoring
- Layer 4: ML Judgment Priors - Learn which knowledge matters

### Knowledge Integration System (KIS)

Every decision uses KIS to synthesize knowledge:
- 5-factor scoring: Domain weight × Type weight × Memory weight × Context weight × Goal alignment
- 40K+ doctrine items across 19 domains
- Confidence scoring on all outputs

---

## Troubleshooting

### "Ollama not running"
```bash
# In separate terminal:
ollama serve

# Or check if running:
ollama list
```

### "Module not found" errors
```bash
# Ensure you're in the era directory:
cd c:\era

# And virtual environment is active:
.\.venv\Scripts\Activate.ps1
```

### "ModuleNotFoundError: No module named..."
```bash
# Reinstall requirements:
pip install -r requirements.txt
```

### Conversation takes too long
- Deepseek-r1:8b can be slow (2-3 min per response)
- Use fewer rounds: `--rounds 2` instead of `--rounds 5`
- Or use faster model (requires code modification)

### Port conflicts
- Ollama uses port 11434 by default
- If port unavailable, Ollama will fail to start
- Check: `netstat -ano | findstr 11434`

---

## Example Workflows

### Example 1: Quick LLM Demo (2 minutes)
```bash
python llm_conversation.py --mode demo --rounds 2
```
→ See a 2-round conversation on intelligent decision systems

### Example 2: Interactive Topic Exploration (5-10 minutes)
```bash
python llm_conversation.py
# Topic: AI ethics in healthcare
# Rounds: 3
```
→ Explore your topic with back-and-forth dialogue

### Example 3: Full Decision Guidance (10-20 minutes)
```bash
python system_main.py
# Problem: "I'm overwhelmed at work and considering changing careers"
```
→ Get 9-phase guidance with clarifying questions, council input, and satisfaction assessment

### Example 4: Session-Based Problem Solving (Variable)
```bash
python run_session_conversation.py
# First session: Immediate problem
# Follow-up sessions: Next steps, concerns, implementation
```
→ Track related problems across multiple sessions with automatic mode escalation

---

## Performance Notes

**Typical Times:**
- 1-round demo: ~30 seconds
- 3-round demo: ~3-5 minutes
- Full 9-phase decision: ~10-20 minutes (depending on mode & clarifications)

**System Resources:**
- RAM: ~2GB minimum (LLM inference)
- Disk: ~1GB for models, ~2GB for data
- Network: Not required (Ollama is local)

---

## Next Steps

1. **First Time?** Run a demo:
   ```bash
   python llm_conversation.py --mode demo --rounds 1
   ```

2. **Want to Explore Topics?** Go interactive:
   ```bash
   python llm_conversation.py
   ```

3. **Need Real Decision Guidance?** Use system_main:
   ```bash
   python system_main.py
   ```

4. **Want to See Full Capabilities?** Check documentation:
   - [MODE_SELECTION_GUIDE.md](documentation/MODE_SELECTION_GUIDE.md) - When to use which mode
   - [SESSION_FEATURES_GUIDE.md](documentation/SESSION_FEATURES_GUIDE.md) - Multi-session problem solving
   - [SYSTEM_ARCHITECTURE.md](documentation/SYSTEM_ARCHITECTURE.md) - Complete architecture overview

---

## Project Structure

```
c:\era\
├── llm_conversation.py          # LLM dialogue engine (START HERE!)
├── system_main.py               # Decision guidance system
├── run_session_conversation.py   # Session-based problem solving
├── sovereign_main.py             # Ministerial council simulation
├── START_HERE.md                # This file
├── CHANGELOG.md                 # Version history
├── requirements.txt             # Python dependencies
├── data/                        # All data (sessions, docs, memory)
├── persona/                     # Interactive persona system
├── sovereign/                   # Ministerial council
├── ml/                          # Machine learning wisdom
├── tests/                       # Test suite
├── ingestion/                   # Knowledge ingestion pipeline
├── documentation/               # Comprehensive guides (50+ files)
└── hse/                         # Human Simulation Engine
```

---

## Support & Documentation

- **Architecture Deep Dive:** [SYSTEM_ARCHITECTURE.md](documentation/SYSTEM_ARCHITECTURE.md)
- **Mode Selection:** [MODE_SELECTION_GUIDE.md](documentation/MODE_SELECTION_GUIDE.md)
- **Session Features:** [SESSION_FEATURES_GUIDE.md](documentation/SESSION_FEATURES_GUIDE.md)
- **All Documentation:** See [documentation/](documentation/) folder (50+ guides)

---

## Version History

See [CHANGELOG.md](CHANGELOG.md) for complete version history and changes.

---

**Last Updated:** February 19, 2026
**Status:** ✅ System Operational - All core features verified and tested
