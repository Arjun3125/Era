"""
ML-Integrated Conversation System: Verification & Quick Test

Verifies all components are properly integrated and working.
"""

import sys
from pathlib import Path

# Setup path
sys.path.insert(0, str(Path(__file__).parent))


def verify_imports():
    """Verify all required imports work."""
    print("\n" + "="*70)
    print("1Ô∏è‚É£  VERIFYING IMPORTS")
    print("="*70 + "\n")
    
    checks = []
    
    # LLM components
    try:
        from persona.ollama_runtime import OllamaRuntime
        checks.append(("‚úÖ", "OllamaRuntime", "persona/ollama_runtime.py"))
    except Exception as e:
        checks.append(("‚ùå", "OllamaRuntime", str(e)))
    
    # Domain detection
    try:
        from persona.domain_detector import analyze_situation
        checks.append(("‚úÖ", "Domain Detector", "persona/domain_detector.py"))
    except Exception as e:
        checks.append(("‚ùå", "Domain Detector", str(e)))
    
    # Session management
    try:
        from persona.session_manager import SessionManager
        checks.append(("‚úÖ", "SessionManager", "persona/session_manager.py"))
    except Exception as e:
        checks.append(("‚ùå", "SessionManager", str(e)))
    
    # Learning components
    try:
        from persona.learning.episodic_memory import EpisodicMemory, Episode
        checks.append(("‚úÖ", "EpisodicMemory", "persona/learning/episodic_memory.py"))
    except Exception as e:
        checks.append(("‚ùå", "EpisodicMemory", str(e)))
    
    try:
        from persona.learning.performance_metrics import PerformanceMetrics
        checks.append(("‚úÖ", "PerformanceMetrics", "persona/learning/performance_metrics.py"))
    except Exception as e:
        checks.append(("‚ùå", "PerformanceMetrics", str(e)))
    
    # ML components
    try:
        from ml.ml_orchestrator import MLWisdomOrchestrator
        checks.append(("‚úÖ", "MLWisdomOrchestrator", "ml/ml_orchestrator.py"))
    except Exception as e:
        checks.append(("‚ùå", "MLWisdomOrchestrator", str(e)))
    
    try:
        from persona.modes.mode_orchestrator import ModeOrchestrator
        checks.append(("‚úÖ", "ModeOrchestrator", "persona/modes/mode_orchestrator.py"))
    except Exception as e:
        checks.append(("‚ùå", "ModeOrchestrator", str(e)))
    
    # Knowledge engine
    try:
        from persona.knowledge_engine import synthesize_knowledge
        checks.append(("‚úÖ", "KnowledgeEngine", "persona/knowledge_engine.py"))
    except Exception as e:
        checks.append(("‚ùå", "KnowledgeEngine", str(e)))
    
    # Main system
    try:
        from ml_integrated_conversation import MLIntegratedConversation
        checks.append(("‚úÖ", "MLIntegratedConversation", "ml_integrated_conversation.py"))
    except Exception as e:
        checks.append(("‚ùå", "MLIntegratedConversation", str(e)))
    
    for status, component, source in checks:
        print(f"  {status} {component:30} {source}")
    
    passed = len([c for c in checks if c[0] == "‚úÖ"])
    total = len(checks)
    print(f"\n  {passed}/{total} imports successful")
    
    return passed == total


def verify_directories():
    """Verify required directories exist."""
    print("\n" + "="*70)
    print("2Ô∏è‚É£  VERIFYING DIRECTORIES")
    print("="*70 + "\n")
    
    required_dirs = [
        "data/sessions",
        "data/conversations",
        "data/memory",
        "data/learning_insights",
        "ml/cache",
        "persona/learning",
        "persona/modes",
    ]
    
    checks = []
    for directory in required_dirs:
        path = Path(directory)
        if path.exists():
            checks.append(("‚úÖ", directory))
        else:
            # Try to create it
            try:
                path.mkdir(parents=True, exist_ok=True)
                checks.append(("‚úÖ", f"{directory} (created)"))
            except Exception as e:
                checks.append(("‚ùå", f"{directory}: {e}"))
    
    for status, directory in checks:
        print(f"  {status} {directory}")
    
    passed = len([c for c in checks if c[0] == "‚úÖ"])
    total = len(checks)
    print(f"\n  {passed}/{total} directories ready")
    
    return passed == total


def test_llm_connection():
    """Test LLM connectivity."""
    print("\n" + "="*70)
    print("3Ô∏è‚É£  TESTING LLM CONNECTIVITY")
    print("="*70 + "\n")
    
    try:
        from persona.ollama_runtime import OllamaRuntime
        
        print("  Testing connection to Ollama...")
        llm = OllamaRuntime(
            speak_model="qwen3:14b",
            analyze_model="qwen3:14b"
        )
        
        test_prompt = "Respond briefly with 'OK' if you can read this."
        response = llm.analyze(
            system_prompt="You are a simple test assistant.",
            user_prompt=test_prompt
        )
        
        if response and len(response) > 0:
            print(f"  ‚úÖ Ollama connected")
            print(f"  ‚úÖ Model response: {response[:50]}...")
            return True
        else:
            print(f"  ‚ùå No response from model")
            return False
            
    except Exception as e:
        print(f"  ‚ùå LLM connection failed: {e}")
        print("\n     Make sure Ollama is running:")
        print("     $ ollama serve")
        return False


def test_domain_detection():
    """Test domain detection."""
    print("\n" + "="*70)
    print("4Ô∏è‚É£  TESTING DOMAIN DETECTION")
    print("="*70 + "\n")
    
    try:
        from persona.domain_detector import analyze_situation
        
        test_problem = "I'm considering a career change but worried about financial impact."
        
        print(f"  Testing: \"{test_problem}\"")
        analysis = analyze_situation(test_problem, llm_adapter=None)
        
        domains = analysis.get("domains", [])
        stakes = analysis.get("stakes", "unknown")
        
        print(f"  ‚úÖ Domain detection works")
        print(f"     Domains: {', '.join(domains)}")
        print(f"     Stakes: {stakes}")
        
        return len(domains) > 0
        
    except Exception as e:
        print(f"  ‚ùå Domain detection failed: {e}")
        return False


def test_session_manager():
    """Test session manager."""
    print("\n" + "="*70)
    print("5Ô∏è‚É£  TESTING SESSION MANAGER")
    print("="*70 + "\n")
    
    try:
        from persona.session_manager import SessionManager
        
        manager = SessionManager()
        
        session = manager.start_session(
            problem_statement="Test problem",
            domains=["career"],
            domain_confidence=0.85,
            stakes="medium",
            reversibility="reversible"
        )
        
        print(f"  ‚úÖ SessionManager works")
        print(f"     Session ID: {session.session_id if hasattr(session, 'session_id') else 'Session object'}")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå SessionManager failed: {e}")
        return False


def test_learning_components():
    """Test learning components."""
    print("\n" + "="*70)
    print("6Ô∏è‚É£  TESTING LEARNING COMPONENTS")
    print("="*70 + "\n")
    
    checks = []
    
    # Test episodic memory
    try:
        from persona.learning.episodic_memory import EpisodicMemory, Episode
        
        memory = EpisodicMemory()
        episode = Episode(
            episode_id="test_episode",
            turn_id=1,
            domain="career",
            user_input="Test input",
            persona_recommendation="Test recommendation",
            confidence=0.85,
            minister_stance="Test",
            council_recommendation="Test outcome",
            outcome="success",
            regret_score=0.0
        )
        
        memory.store_episode(episode)
        checks.append(("‚úÖ", "EpisodicMemory"))
        
    except Exception as e:
        checks.append(("‚ùå", f"EpisodicMemory: {e}"))
    
    # Test performance metrics
    try:
        from persona.learning.performance_metrics import PerformanceMetrics
        
        metrics = PerformanceMetrics()
        metrics.record_decision(
            turn=1,
            domain="career",
            recommendation="Test",
            confidence=0.85,
            outcome="success",
            regret=0.0
        )
        
        checks.append(("‚úÖ", "PerformanceMetrics"))
        
    except Exception as e:
        checks.append(("‚ùå", f"PerformanceMetrics: {e}"))
    
    for status, component in checks:
        print(f"  {status} {component}")
    
    passed = len([c for c in checks if c[0] == "‚úÖ"])
    return passed == len(checks)


def print_summary(results):
    """Print verification summary."""
    print("\n" + "="*70)
    print("‚úÖ VERIFICATION SUMMARY")
    print("="*70 + "\n")
    
    total = len(results)
    passed = len([r for r in results if r])
    
    print(f"  {passed}/{total} verification checks passed")
    
    if passed == total:
        print("\n  üéâ All systems ready!")
        print("\n  Next step:")
        print("    python ml_integrated_conversation.py")
    else:
        print("\n  ‚ö†Ô∏è  Some checks failed. Review output above.")
        print("     Make sure Ollama is running: ollama serve")
    
    print("\n" + "="*70 + "\n")


def main():
    """Run all verifications."""
    print("\n")
    print("‚ïî" + "="*68 + "‚ïó")
    print("‚ïë" + " "*15 + "ML-INTEGRATED CONVERSATION SYSTEM" + " "*21 + "‚ïë")
    print("‚ïë" + " "*20 + "Verification & Setup Check" + " "*22 + "‚ïë")
    print("‚ïö" + "="*68 + "‚ïù")
    
    results = [
        verify_imports(),
        verify_directories(),
    ]
    
    # Optional tests (don't fail if LLM not available)
    print("\n[Optional Tests - require Ollama running]\n")
    try:
        results.append(test_llm_connection())
    except:
        print("  ‚ö†Ô∏è  LLM test skipped (Ollama may not be running)")
        results.append(None)
    
    try:
        results.append(test_domain_detection())
    except:
        results.append(False)
    
    try:
        results.append(test_session_manager())
    except:
        results.append(False)
    
    try:
        results.append(test_learning_components())
    except:
        results.append(False)
    
    # Remove None values from optional tests
    results = [r for r in results if r is not None]
    
    print_summary(results)


if __name__ == "__main__":
    main()
